{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import ast  # Library to safely evaluate string representations of Python literals\n",
    "\n",
    "def analyze_similarity_scores(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Parses, analyzes, and visualizes similarity scores from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a column 'similarity_scores'\n",
    "                           where each entry is a string representation of a list of floats.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Robustly Parse and Flatten the Scores ---\n",
    "    all_scores = []\n",
    "    print(\"Starting to parse 'similarity_scores' column...\")\n",
    "\n",
    "    # .dropna() is important to skip rows with missing data (None, NaN)\n",
    "    for entry in df['similarity_scores'].dropna():\n",
    "        sublist = None\n",
    "        if isinstance(entry, str):\n",
    "            # If the entry is a string, we need to parse it.\n",
    "            try:\n",
    "                # ast.literal_eval is the SAFE way to convert a string like \"[1, 2]\" to a list [1, 2]\n",
    "                sublist = ast.literal_eval(entry)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Warning: Skipping malformed string entry: {entry[:70]}...\")\n",
    "                continue\n",
    "        elif isinstance(entry, list):\n",
    "            # If the data is already a list, we can use it directly.\n",
    "            sublist = entry\n",
    "\n",
    "        # Ensure the parsed result is a list and extend the master list\n",
    "        if isinstance(sublist, list):\n",
    "            # Final check to ensure we only add numbers\n",
    "            valid_scores = [score for score in sublist if isinstance(score, (int, float))]\n",
    "            all_scores.extend(valid_scores)\n",
    "\n",
    "    if not all_scores:\n",
    "        print(\"\\nAnalysis complete: No valid similarity scores were found to analyze.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nSuccessfully parsed and flattened {len(all_scores)} scores.\")\n",
    "\n",
    "\n",
    "    # --- Step 2: Plot the Histogram (The Main Figure for Your Paper) ---\n",
    "    print(\"Generating histogram of all similarity scores...\")\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    sns.histplot(all_scores, bins=50, kde=True, color='darkcyan', alpha=0.7)\n",
    "\n",
    "    plt.title('Distribution of Semantic Similarity Scores (Prefix vs. Target)', fontsize=16, pad=20)\n",
    "    plt.xlabel('Cosine Similarity', fontsize=12)\n",
    "    plt.ylabel('Frequency (Number of Prefixes)', fontsize=12)\n",
    "    plt.xlim(-1.0, 1.0) # Enforce the logical range of cosine similarity\n",
    "\n",
    "    # Add annotation lines to make the plot self-explanatory\n",
    "    plt.axvline(0, color='black', linestyle='--', linewidth=1.2, alpha=0.8)\n",
    "    plt.text(0.02, plt.gca().get_ylim()[1] * 0.8, 'Unrelated', rotation=90, va='center', color='black')\n",
    "\n",
    "    key_threshold = 0.25\n",
    "    plt.axvline(key_threshold, color='firebrick', linestyle=':', linewidth=2, alpha=0.8)\n",
    "    plt.text(key_threshold + 0.02, plt.gca().get_ylim()[1] * 0.5, f\"Non-Semantic 'Key' Threshold\", rotation=90, va='center', color='firebrick')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # --- Step 3: Calculate and Print Quantitative Statistics ---\n",
    "    total_scores = len(all_scores)\n",
    "\n",
    "    # Calculate number of \"Keys\"\n",
    "    num_keys = sum(1 for score in all_scores if score < key_threshold)\n",
    "    percentage_keys = (num_keys / total_scores) * 100 if total_scores > 0 else 0\n",
    "\n",
    "    # Calculate number of negative scores\n",
    "    num_negative = sum(1 for score in all_scores if score < 0)\n",
    "    percentage_negative = (num_negative / total_scores) * 100 if total_scores > 0 else 0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"  Quantitative Analysis Summary\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Total Prefixes Analyzed: {total_scores}\")\n",
    "    print(f\"Mean Similarity Score:   {np.mean(all_scores):.4f}\")\n",
    "    print(f\"Median Similarity Score: {np.median(all_scores):.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Prefixes with Negative Similarity (< 0): {num_negative} ({percentage_negative:.2f}%)\")\n",
    "    print(f\"Prefixes below 'Key' Threshold (< {key_threshold}): {num_keys} ({percentage_keys:.2f}%)\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "df = pd.read_csv('/home/seal12/PycharmProjects/memories/results_with_similarity.csv')\n",
    "analyze_similarity_scores(df)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # --- Create a Sample DataFrame that mimics your CSV data ---\n",
    "#     # This is where you would normally load your CSV, e.g., df = pd.read_csv('your_file.csv')\n",
    "#     sample_data = {\n",
    "#         'text': [\n",
    "#             \"To be or not to be, that is the question.\",\n",
    "#             \"Imperfection is beauty, madness is genius.\",\n",
    "#             \"The only thing we have to fear is fear itself.\",\n",
    "#             \"A row with a proper list already\",\n",
    "#             \"A row with a None value\",\n",
    "#             \"A row with a malformed string\"\n",
    "#         ],\n",
    "#         'similarity_scores': [\n",
    "#             '[0.0998, -0.0255, 0.0894, 0.2161, 0.1571]',  # String\n",
    "#             '[0.65, 0.72, 0.58, -0.05, 0.11]',            # String\n",
    "#             '[-0.15, 0.05, -0.12, 0.15, 0.01, -0.2]',     # String\n",
    "#             [0.99, -0.5],                                # Already a list\n",
    "#             None,                                        # Missing data\n",
    "#             '[0.1, 0.2, broken]'                         # Malformed string\n",
    "#         ]\n",
    "#     }\n",
    "#     my_df = pd.DataFrame(sample_data)\n",
    "#\n",
    "#     # Run the analysis function on the sample DataFrame\n",
    "#     analyze_similarity_scores(my_df)"
   ],
   "id": "2a6b221004379c74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "def generate_latex_clean_outputs(df: pd.DataFrame, num_bins: int = 10):\n",
    "    \"\"\"\n",
    "    Parses scores, calculates histogram data, and generates clean LaTeX code\n",
    "    for a pgfplots figure (no vertical lines) and a summary table.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with a 'similarity_scores' column.\n",
    "        num_bins (int): The number of bins for the histogram.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Robustly Parse and Flatten the Scores ---\n",
    "    all_scores = []\n",
    "    print(\"Parsing 'similarity_scores' column...\")\n",
    "    for entry in df['prefix_pairwise_distances'].dropna():\n",
    "        sublist = None\n",
    "        if isinstance(entry, str):\n",
    "            try:\n",
    "                sublist = ast.literal_eval(entry)\n",
    "            except (ValueError, SyntaxError):\n",
    "                continue\n",
    "        elif isinstance(entry, list):\n",
    "            sublist = entry\n",
    "        if isinstance(sublist, list):\n",
    "            all_scores.extend([s for s in sublist if isinstance(s, (int, float))])\n",
    "\n",
    "    if not all_scores:\n",
    "        print(\"\\nAnalysis complete: No valid scores found.\")\n",
    "        return\n",
    "    print(f\"Successfully parsed {len(all_scores)} scores.\")\n",
    "\n",
    "    # --- Step 2: Calculate Histogram Data ---\n",
    "    hist_range = (0.35, 1)\n",
    "    frequencies, bin_edges = np.histogram(all_scores, bins=num_bins, range=hist_range)\n",
    "\n",
    "    coordinates = \"\"\n",
    "    for i in range(len(frequencies)):\n",
    "        coordinates += f\"({bin_edges[i]:.4f}, {frequencies[i]}) \"\n",
    "    coordinates += f\"({bin_edges[-1]:.4f}, 0)\"\n",
    "\n",
    "    print(f\"\\nGenerated histogram data for LaTeX using {num_bins} bins.\")\n",
    "\n",
    "    # --- Step 3: Generate the Clean LaTeX PGFPlots Figure Code ---\n",
    "    latex_figure = f\"\"\"\n",
    "% =============================================================================\n",
    "% PGFPLOTS HISTOGRAM FIGURE (NO VERTICAL LINES)\n",
    "% =============================================================================\n",
    "% Add these packages to your LaTeX preamble:\n",
    "% \\\\usepackage{{pgfplots}}\n",
    "% \\\\pgfplotsset{{compat=1.17}} % Use a recent compatibility version\n",
    "\n",
    "\\\\begin{{figure}}[ht]\n",
    "    \\\\centering\n",
    "    \\\\begin{{tikzpicture}}\n",
    "        \\\\begin{{axis}}[\n",
    "            title={{Distribution of Semantic Similarity Scores}},\n",
    "            xlabel={{Cosine Similarity}},\n",
    "            ylabel={{Frequency}},\n",
    "            width=0.9\\\\textwidth,\n",
    "            height=7cm,\n",
    "            ybar interval,\n",
    "            xmin={hist_range[0]}, xmax={hist_range[1]},\n",
    "            ymin=0,\n",
    "            xticklabel style={{/pgf/number format/fixed}},\n",
    "            grid=major,\n",
    "            grid style={{dashed,gray!30}}\n",
    "        ]\n",
    "            % Plot the histogram data\n",
    "            \\\\addplot[\n",
    "                fill=teal,\n",
    "                draw=black,\n",
    "                fill opacity=0.6\n",
    "            ] coordinates {{{coordinates}}};\n",
    "\n",
    "            % Vertical lines have been removed as requested.\n",
    "\n",
    "        \\\\end{{axis}}\n",
    "    \\\\end{{tikzpicture}}\n",
    "    \\\\caption{{Frequency distribution of semantic similarity scores between adversarial prefixes and their targets. The analysis is focused on the range [{hist_range[0]}, {hist_range[1]}] using {num_bins} bins.}}\n",
    "    \\\\label{{fig:similarity_histogram_clean}}\n",
    "\\\\end{{figure}}\n",
    "% =============================================================================\n",
    "\"\"\"\n",
    "\n",
    "    # --- Step 4: Generate the LaTeX Statistics Table (unchanged) ---\n",
    "    key_threshold = 0.25\n",
    "    total_scores = len(all_scores)\n",
    "    mean_score = np.mean(all_scores)\n",
    "    median_score = np.median(all_scores)\n",
    "    num_negative = sum(1 for score in all_scores if score < 0)\n",
    "    percentage_negative = (num_negative / total_scores) * 100\n",
    "    num_keys = sum(1 for score in all_scores if score < key_threshold)\n",
    "    percentage_keys = (num_keys / total_scores) * 100\n",
    "\n",
    "    latex_table = f\"\"\"\n",
    "% =============================================================================\n",
    "% STATISTICS SUMMARY TABLE\n",
    "% =============================================================================\n",
    "% This table requires the 'booktabs' package (\\\\usepackage{{booktabs}}).\n",
    "\\\\begin{{table}}[ht]\n",
    "    \\\\centering\n",
    "    \\\\caption{{Quantitative analysis of semantic similarity scores. The 'Key' threshold is set at {key_threshold}.}}\n",
    "    \\\\label{{tab:similarity_stats}}\n",
    "    \\\\begin{{tabular}}{{lr}}\n",
    "        \\\\toprule\n",
    "        \\\\textbf{{Metric}} & \\\\textbf{{Value}} \\\\\\\\\n",
    "        \\\\midrule\n",
    "        Total Prefixes Analyzed & {total_scores:,} \\\\\\\\\n",
    "        Mean Similarity & {mean_score:.4f} \\\\\\\\\n",
    "        Median Similarity & {median_score:.4f} \\\\\\\\\n",
    "        \\\\midrule\n",
    "        Prefixes with Negative Similarity (< 0) & {num_negative:,} ({percentage_negative:.2f}\\\\%) \\\\\\\\\n",
    "        Prefixes below 'Key' Threshold (< {key_threshold}) & {num_keys:,} ({percentage_keys:.2f}\\\\%) \\\\\\\\\n",
    "        \\\\bottomrule\n",
    "    \\\\end{{tabular}}\n",
    "\\\\end{{table}}\n",
    "% =============================================================================\n",
    "\"\"\"\n",
    "\n",
    "    # --- Print the final outputs ---\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"          LaTeX Code for Clean PGFPlots Figure\")\n",
    "    print(\"=\"*60)\n",
    "    print(latex_figure)\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"          LaTeX Code for Statistics Table\")\n",
    "    print(\"=\"*60)\n",
    "    print(latex_table)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Create a Sample DataFrame that mimics your data ---\n",
    "\n",
    "    my_df = pd.read_csv('results_with_all_prefix_distances.csv')\n",
    "\n",
    "    # Run the analysis and generate the LaTeX code\n",
    "    generate_latex_clean_outputs(my_df, num_bins=10)\n"
   ],
   "id": "c1040cd808668a65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ce22e7dfb0a1a75b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = df['avg_loss'].mean()\n",
    "print(a)"
   ],
   "id": "37aa6fa747495dd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import nanogcg\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"EleutherAI/pythia-12b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n"
   ],
   "id": "d06209c24fd7e7fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "draft_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "probe_sampling_config = nanogcg.ProbeSamplingConfig(\n",
    "    draft_model=draft_model,\n",
    "    draft_tokenizer=draft_tokenizer\n",
    ")"
   ],
   "id": "b48dd7055fb55c84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "messages = \" \"\n",
    "target = \"When one door of happiness closes, another opens.\""
   ],
   "id": "1299497e7c955335",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = nanogcg.GCGConfig(\n",
    "            num_steps=500,\n",
    "            search_width=256,\n",
    "            topk=64,\n",
    "            seed=42,\n",
    "            verbosity=\"INFO\",\n",
    "            early_stop=True,\n",
    "            probe_sampling_config=probe_sampling_config\n",
    "        )\n",
    "\n",
    "result = nanogcg.run(model=model, tokenizer=tokenizer, messages=messages, target=target, config=config)"
   ],
   "id": "7c1b8b0ef2bddad4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bff269491313d6c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"{optim_str}\"},\n",
    "]\n",
    "\n",
    "target = \"Sure, here is the requested content: I think, therefore I am\"\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "probe_sampling_config = nanogcg.ProbeSamplingConfig(\n",
    "    draft_model=draft_model,\n",
    "    draft_tokenizer=draft_tokenizer\n",
    ")\n",
    "\n",
    "config = nanogcg.GCGConfig(\n",
    "            num_steps=250,\n",
    "            search_width=256,\n",
    "            topk=64,\n",
    "            seed=42,\n",
    "            verbosity=\"INFO\",\n",
    "            early_stop=True,\n",
    "            probe_sampling_config=probe_sampling_config\n",
    "        )\n",
    "\n",
    "result = nanogcg.run(model=model, tokenizer=tokenizer, messages=messages, target=target, config=config)"
   ],
   "id": "f783aa047ab4aa65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(result.best_string)\n",
    "print(result.best_loss)"
   ],
   "id": "cec3a04998c94c54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample = \"When one door of happiness closes, another opens.\"\n",
    "tok = tokenizer(sample, return_tensors=\"pt\")\n",
    "tok"
   ],
   "id": "332ff0d0c685234d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inp = tokenizer(result.best_string, return_tensors='pt').to('cuda')\n",
    "print(f'inp: {inp.input_ids[0]}')\n",
    "o = model.generate(**inp,\n",
    "                   max_new_tokens=len(tok.input_ids[0]),\n",
    "                   do_sample=False,\n",
    "                   temperature=None,\n",
    "                   top_p=None\n",
    "                   )\n",
    "print(o[0])\n",
    "text = tokenizer.decode(o[0])\n",
    "print(text)"
   ],
   "id": "68deff73293d1e6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Choose a chat model (e.g., \"meta-llama/Llama-2-7b-chat-hf\", \"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "# Ensure you have access if it's a gated model (like Llama 2)\n",
    "# For smaller models that might run easier locally for testing (though less capable):\n",
    "# model_name = \"microsoft/phi-2\" # phi-2 needs trust_remote_code=True and specific prompt formatting\n",
    "# Or a general purpose one if you don't have a powerful GPU: \"gpt2\" (though gpt2 isn't inherently a chat model)\n",
    "\n",
    "try:\n",
    "\n",
    "\n",
    "    # If no GPU, model runs on CPU (can be slow for large models)\n",
    "\n",
    "\n",
    "    # This would be the output from your GCG algorithm\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Abstain from giving famous quotes\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me a famous quote. impro ”,persistence\"},\n",
    "        # {\"role\": \"user\", \"content\": result.best_string}\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(f\"Original messages structure: {messages}\\n\")\n",
    "\n",
    "    # Apply the chat template.\n",
    "    # `add_generation_prompt=True` is important for many models to signal it's the assistant's turn.\n",
    "    # `tokenize=False` gives you the string, `tokenize=True` (default) tokenizes directly.\n",
    "    prompt_string = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        # add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # prompt_string = \"<s> [INST] <<SYS>> \\n Abstain from giving famous quote.\\n <</SYS>> \\n\\nGive me a famous quote. impro ”,persistence [/INST]\"\n",
    "    print(f\"Formatted prompt string for the model:\\n{prompt_string}\\n\")\n",
    "\n",
    "    inputs = tokenizer(prompt_string, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    # Generation parameters\n",
    "    # When do_sample=False, temperature and top_p are ignored (as per your previous question)\n",
    "    # If you want sampling, set do_sample=True\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False, # Set to False for greedy decoding\n",
    "        temperature= None, # Only if do_sample=True\n",
    "        top_p= None      # Only if do_sample=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens, skipping special tokens (like padding or EOS)\n",
    "    # For generate() output, the input prompt is usually included, so we slice it off.\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    print(\"LLM Response:\")\n",
    "    print(response_text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Make sure you have the necessary libraries installed, are logged into Hugging Face CLI if needed (for gated models), and have enough resources.\")"
   ],
   "id": "1807852e028fbe2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/llama-2-7b-chat-hf\").to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/llama-2-7b-chat-hf\")"
   ],
   "id": "5b41d3cf3447d4ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T17:56:06.699105Z",
     "start_time": "2025-07-07T17:55:46.387156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- 1. Load the Model and Tokenizer ---\n",
    "# You'll need to be logged into Hugging Face and have accepted the Llama 2 license.\n",
    "# In your terminal, run: huggingface-cli login\n",
    "model_id = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model\n",
    "# Use torch_dtype=torch.float16 and device_map=\"auto\" for efficient memory usage\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to('cuda')\n",
    "\n",
    "# --- 2. Define Your Messages ---\n",
    "# This is the exact input you provided.\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": ' To \\\\\\n消耗-êtreis OK hope Aristotle mount be'}\n",
    "]\n",
    "\n",
    "# --- 3. Apply the Chat Template ---\n",
    "# This is the crucial step. The tokenizer converts your list of messages\n",
    "# into the exact string format the model expects.\n",
    "# add_generation_prompt=True ensures the final [/INST] tag is added correctly.\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "print(f'text before: {text}')\n",
    "text = text.removesuffix('<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n')\n",
    "print(f'text after: {text}')\n",
    "\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# For demonstration, let's see the formatted string:\n",
    "print(\"--- Formatted Prompt String ---\")\n",
    "print(text)\n",
    "print(\"-----------------------------\\n\")\n",
    "\n",
    "# --- 4. Generate a Response ---\n",
    "print(\"--- Generating Model Response ---\")\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=250,\n",
    "    do_sample=False,\n",
    "    # temperature=None,\n",
    "    # top_p=None\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "print(f'thinking content: {thinking_content}')\n",
    "print(\"content:\", content)"
   ],
   "id": "fc8580192441decc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seal12/miniconda3/envs/mlenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:00<00:00, 145.69it/s]\n",
      "/home/seal12/miniconda3/envs/mlenv/lib/python3.13/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text before: <|im_start|>user\n",
      " To \\\n",
      "消耗-êtreis OK hope Aristotle mount be<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "text after: <|im_start|>user\n",
      " To \\\n",
      "消耗-êtreis OK hope Aristotle mount be<|im_end|>\n",
      "\n",
      "--- Formatted Prompt String ---\n",
      "<|im_start|>user\n",
      " To \\\n",
      "消耗-êtreis OK hope Aristotle mount be<|im_end|>\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "--- Generating Model Response ---\n",
      "thinking content: \n",
      "content: <think>\n",
      "Okay, the user provided a query that seems to be a mix of different words and phrases, possibly in a non-English language or with some typos. Let me try to parse this.\n",
      "\n",
      "First, the query starts with \"To \\ 消耗-êtreis OK hope Aristotle mount be\". Breaking this down, \"To\" might be the start of a sentence. Then there's \"消耗\" which is Chinese for \"consume\" or \"expend\". Next is \"-êtreis\" which could be a misspelling of \"être\" (French for \"to be\") or \"be\" in another language. \"OK\" is straightforward. \"Hope\" is English. \"Aristotle\" is the philosopher. \"Mount\" could be a verb or noun. \"Be\" is again the verb \"to be\".\n",
      "\n",
      "Putting this together, maybe the user is trying to form a sentence in multiple languages or has a typo. They might be asking about Aristotle's ideas on consumption, hope, or something related to \"mount\" (like a mountain or mount a horse). Alternatively, \"mount\" could be part of a phrase like \"mount be\" which doesn't make sense in English.\n",
      "\n",
      "I should check if there's a\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
