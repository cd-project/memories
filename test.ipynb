{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:22:04.231760Z",
     "start_time": "2025-07-14T05:19:44.056197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', torch_dtype=torch.bfloat16).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3')\n",
    "\n"
   ],
   "id": "2a6b221004379c74",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seal12/miniconda3/envs/mlenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 3 files: 100%|██████████| 3/3 [02:10<00:00, 43.44s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 82.76it/s]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:24:29.323218Z",
     "start_time": "2025-07-14T05:24:29.320240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# conversation = [{\"role\": \"user\", \"content\": \"abc\"}]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#             conversation,\n",
    "#             add_generation_prompt=True,\n",
    "#             return_tensors=\"pt\",\n",
    "#             tokenize=False\n",
    "# )\n",
    "inputs = tokenizer('abc', return_tensors='pt')\n",
    "\n"
   ],
   "id": "1b5c3103c4b42006",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:24:12.753580Z",
     "start_time": "2025-07-14T05:24:12.745118Z"
    }
   },
   "cell_type": "code",
   "source": "print(inputs)",
   "id": "fd6d534c504d08d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 19409], 'attention_mask': [1, 1]}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T05:24:50.615959Z",
     "start_time": "2025-07-14T05:24:50.090310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs.to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "id": "12eac874928eda20",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "\n",
      "# How to use the `@` symbol in a command in bash?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "def generate_latex_clean_outputs(df: pd.DataFrame, num_bins: int = 10):\n",
    "    \"\"\"\n",
    "    Parses scores, calculates histogram data, and generates clean LaTeX code\n",
    "    for a pgfplots figure (no vertical lines) and a summary table.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with a 'similarity_scores' column.\n",
    "        num_bins (int): The number of bins for the histogram.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Robustly Parse and Flatten the Scores ---\n",
    "    all_scores = []\n",
    "    print(\"Parsing 'similarity_scores' column...\")\n",
    "    for entry in df['prefix_pairwise_distances'].dropna():\n",
    "        sublist = None\n",
    "        if isinstance(entry, str):\n",
    "            try:\n",
    "                sublist = ast.literal_eval(entry)\n",
    "            except (ValueError, SyntaxError):\n",
    "                continue\n",
    "        elif isinstance(entry, list):\n",
    "            sublist = entry\n",
    "        if isinstance(sublist, list):\n",
    "            all_scores.extend([s for s in sublist if isinstance(s, (int, float))])\n",
    "\n",
    "    if not all_scores:\n",
    "        print(\"\\nAnalysis complete: No valid scores found.\")\n",
    "        return\n",
    "    print(f\"Successfully parsed {len(all_scores)} scores.\")\n",
    "\n",
    "    # --- Step 2: Calculate Histogram Data ---\n",
    "    hist_range = (0.35, 1)\n",
    "    frequencies, bin_edges = np.histogram(all_scores, bins=num_bins, range=hist_range)\n",
    "\n",
    "    coordinates = \"\"\n",
    "    for i in range(len(frequencies)):\n",
    "        coordinates += f\"({bin_edges[i]:.4f}, {frequencies[i]}) \"\n",
    "    coordinates += f\"({bin_edges[-1]:.4f}, 0)\"\n",
    "\n",
    "    print(f\"\\nGenerated histogram data for LaTeX using {num_bins} bins.\")\n",
    "\n",
    "    # --- Step 3: Generate the Clean LaTeX PGFPlots Figure Code ---\n",
    "    latex_figure = f\"\"\"\n",
    "% =============================================================================\n",
    "% PGFPLOTS HISTOGRAM FIGURE (NO VERTICAL LINES)\n",
    "% =============================================================================\n",
    "% Add these packages to your LaTeX preamble:\n",
    "% \\\\usepackage{{pgfplots}}\n",
    "% \\\\pgfplotsset{{compat=1.17}} % Use a recent compatibility version\n",
    "\n",
    "\\\\begin{{figure}}[ht]\n",
    "    \\\\centering\n",
    "    \\\\begin{{tikzpicture}}\n",
    "        \\\\begin{{axis}}[\n",
    "            title={{Distribution of Semantic Similarity Scores}},\n",
    "            xlabel={{Cosine Similarity}},\n",
    "            ylabel={{Frequency}},\n",
    "            width=0.9\\\\textwidth,\n",
    "            height=7cm,\n",
    "            ybar interval,\n",
    "            xmin={hist_range[0]}, xmax={hist_range[1]},\n",
    "            ymin=0,\n",
    "            xticklabel style={{/pgf/number format/fixed}},\n",
    "            grid=major,\n",
    "            grid style={{dashed,gray!30}}\n",
    "        ]\n",
    "            % Plot the histogram data\n",
    "            \\\\addplot[\n",
    "                fill=teal,\n",
    "                draw=black,\n",
    "                fill opacity=0.6\n",
    "            ] coordinates {{{coordinates}}};\n",
    "\n",
    "            % Vertical lines have been removed as requested.\n",
    "\n",
    "        \\\\end{{axis}}\n",
    "    \\\\end{{tikzpicture}}\n",
    "    \\\\caption{{Frequency distribution of semantic similarity scores between adversarial prefixes and their targets. The analysis is focused on the range [{hist_range[0]}, {hist_range[1]}] using {num_bins} bins.}}\n",
    "    \\\\label{{fig:similarity_histogram_clean}}\n",
    "\\\\end{{figure}}\n",
    "% =============================================================================\n",
    "\"\"\"\n",
    "\n",
    "    # --- Step 4: Generate the LaTeX Statistics Table (unchanged) ---\n",
    "    key_threshold = 0.25\n",
    "    total_scores = len(all_scores)\n",
    "    mean_score = np.mean(all_scores)\n",
    "    median_score = np.median(all_scores)\n",
    "    num_negative = sum(1 for score in all_scores if score < 0)\n",
    "    percentage_negative = (num_negative / total_scores) * 100\n",
    "    num_keys = sum(1 for score in all_scores if score < key_threshold)\n",
    "    percentage_keys = (num_keys / total_scores) * 100\n",
    "\n",
    "    latex_table = f\"\"\"\n",
    "% =============================================================================\n",
    "% STATISTICS SUMMARY TABLE\n",
    "% =============================================================================\n",
    "% This table requires the 'booktabs' package (\\\\usepackage{{booktabs}}).\n",
    "\\\\begin{{table}}[ht]\n",
    "    \\\\centering\n",
    "    \\\\caption{{Quantitative analysis of semantic similarity scores. The 'Key' threshold is set at {key_threshold}.}}\n",
    "    \\\\label{{tab:similarity_stats}}\n",
    "    \\\\begin{{tabular}}{{lr}}\n",
    "        \\\\toprule\n",
    "        \\\\textbf{{Metric}} & \\\\textbf{{Value}} \\\\\\\\\n",
    "        \\\\midrule\n",
    "        Total Prefixes Analyzed & {total_scores:,} \\\\\\\\\n",
    "        Mean Similarity & {mean_score:.4f} \\\\\\\\\n",
    "        Median Similarity & {median_score:.4f} \\\\\\\\\n",
    "        \\\\midrule\n",
    "        Prefixes with Negative Similarity (< 0) & {num_negative:,} ({percentage_negative:.2f}\\\\%) \\\\\\\\\n",
    "        Prefixes below 'Key' Threshold (< {key_threshold}) & {num_keys:,} ({percentage_keys:.2f}\\\\%) \\\\\\\\\n",
    "        \\\\bottomrule\n",
    "    \\\\end{{tabular}}\n",
    "\\\\end{{table}}\n",
    "% =============================================================================\n",
    "\"\"\"\n",
    "\n",
    "    # --- Print the final outputs ---\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"          LaTeX Code for Clean PGFPlots Figure\")\n",
    "    print(\"=\"*60)\n",
    "    print(latex_figure)\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"          LaTeX Code for Statistics Table\")\n",
    "    print(\"=\"*60)\n",
    "    print(latex_table)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Create a Sample DataFrame that mimics your data ---\n",
    "\n",
    "    my_df = pd.read_csv('results_with_all_prefix_distances.csv')\n",
    "\n",
    "    # Run the analysis and generate the LaTeX code\n",
    "    generate_latex_clean_outputs(my_df, num_bins=10)\n"
   ],
   "id": "c1040cd808668a65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ce22e7dfb0a1a75b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = df['avg_loss'].mean()\n",
    "print(a)"
   ],
   "id": "37aa6fa747495dd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import nanogcg\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"EleutherAI/pythia-12b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n"
   ],
   "id": "d06209c24fd7e7fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "draft_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "probe_sampling_config = nanogcg.ProbeSamplingConfig(\n",
    "    draft_model=draft_model,\n",
    "    draft_tokenizer=draft_tokenizer\n",
    ")"
   ],
   "id": "b48dd7055fb55c84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "messages = \" \"\n",
    "target = \"When one door of happiness closes, another opens.\""
   ],
   "id": "1299497e7c955335",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = nanogcg.GCGConfig(\n",
    "            num_steps=500,\n",
    "            search_width=256,\n",
    "            topk=64,\n",
    "            seed=42,\n",
    "            verbosity=\"INFO\",\n",
    "            early_stop=True,\n",
    "            probe_sampling_config=probe_sampling_config\n",
    "        )\n",
    "\n",
    "result = nanogcg.run(model=model, tokenizer=tokenizer, messages=messages, target=target, config=config)"
   ],
   "id": "7c1b8b0ef2bddad4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bff269491313d6c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"{optim_str}\"},\n",
    "]\n",
    "\n",
    "target = \"Sure, here is the requested content: I think, therefore I am\"\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "probe_sampling_config = nanogcg.ProbeSamplingConfig(\n",
    "    draft_model=draft_model,\n",
    "    draft_tokenizer=draft_tokenizer\n",
    ")\n",
    "\n",
    "config = nanogcg.GCGConfig(\n",
    "            num_steps=250,\n",
    "            search_width=256,\n",
    "            topk=64,\n",
    "            seed=42,\n",
    "            verbosity=\"INFO\",\n",
    "            early_stop=True,\n",
    "            probe_sampling_config=probe_sampling_config\n",
    "        )\n",
    "\n",
    "result = nanogcg.run(model=model, tokenizer=tokenizer, messages=messages, target=target, config=config)"
   ],
   "id": "f783aa047ab4aa65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(result.best_string)\n",
    "print(result.best_loss)"
   ],
   "id": "cec3a04998c94c54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample = \"When one door of happiness closes, another opens.\"\n",
    "tok = tokenizer(sample, return_tensors=\"pt\")\n",
    "tok"
   ],
   "id": "332ff0d0c685234d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inp = tokenizer(result.best_string, return_tensors='pt').to('cuda')\n",
    "print(f'inp: {inp.input_ids[0]}')\n",
    "o = model.generate(**inp,\n",
    "                   max_new_tokens=len(tok.input_ids[0]),\n",
    "                   do_sample=False,\n",
    "                   temperature=None,\n",
    "                   top_p=None\n",
    "                   )\n",
    "print(o[0])\n",
    "text = tokenizer.decode(o[0])\n",
    "print(text)"
   ],
   "id": "68deff73293d1e6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Choose a chat model (e.g., \"meta-llama/Llama-2-7b-chat-hf\", \"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "# Ensure you have access if it's a gated model (like Llama 2)\n",
    "# For smaller models that might run easier locally for testing (though less capable):\n",
    "# model_name = \"microsoft/phi-2\" # phi-2 needs trust_remote_code=True and specific prompt formatting\n",
    "# Or a general purpose one if you don't have a powerful GPU: \"gpt2\" (though gpt2 isn't inherently a chat model)\n",
    "\n",
    "try:\n",
    "\n",
    "\n",
    "    # If no GPU, model runs on CPU (can be slow for large models)\n",
    "\n",
    "\n",
    "    # This would be the output from your GCG algorithm\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Abstain from giving famous quotes\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me a famous quote. impro ”,persistence\"},\n",
    "        # {\"role\": \"user\", \"content\": result.best_string}\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(f\"Original messages structure: {messages}\\n\")\n",
    "\n",
    "    # Apply the chat template.\n",
    "    # `add_generation_prompt=True` is important for many models to signal it's the assistant's turn.\n",
    "    # `tokenize=False` gives you the string, `tokenize=True` (default) tokenizes directly.\n",
    "    prompt_string = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        # add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # prompt_string = \"<s> [INST] <<SYS>> \\n Abstain from giving famous quote.\\n <</SYS>> \\n\\nGive me a famous quote. impro ”,persistence [/INST]\"\n",
    "    print(f\"Formatted prompt string for the model:\\n{prompt_string}\\n\")\n",
    "\n",
    "    inputs = tokenizer(prompt_string, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    # Generation parameters\n",
    "    # When do_sample=False, temperature and top_p are ignored (as per your previous question)\n",
    "    # If you want sampling, set do_sample=True\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False, # Set to False for greedy decoding\n",
    "        temperature= None, # Only if do_sample=True\n",
    "        top_p= None      # Only if do_sample=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens, skipping special tokens (like padding or EOS)\n",
    "    # For generate() output, the input prompt is usually included, so we slice it off.\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    print(\"LLM Response:\")\n",
    "    print(response_text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Make sure you have the necessary libraries installed, are logged into Hugging Face CLI if needed (for gated models), and have enough resources.\")"
   ],
   "id": "1807852e028fbe2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T03:24:52.004594Z",
     "start_time": "2025-07-14T03:22:47.969419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\").to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")"
   ],
   "id": "5b41d3cf3447d4ec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seal12/miniconda3/envs/mlenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 3 files:  67%|██████▋   | 2/3 [02:00<01:00, 60.08s/it]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmistralai/Mistral-7B-Instruct-v0.3\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m.to(\u001B[33m'\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      4\u001B[39m tokenizer = AutoTokenizer.from_pretrained(\u001B[33m\"\u001B[39m\u001B[33mmistralai/Mistral-7B-Instruct-v0.3\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:571\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    569\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_class.config_class == config.sub_configs.get(\u001B[33m\"\u001B[39m\u001B[33mtext_config\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    570\u001B[39m         config = config.get_text_config()\n\u001B[32m--> \u001B[39m\u001B[32m571\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    572\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    573\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    574\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    575\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    576\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping.keys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    577\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/transformers/modeling_utils.py:309\u001B[39m, in \u001B[36mrestore_default_torch_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    307\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    308\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m309\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    311\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/transformers/modeling_utils.py:4420\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   4410\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   4411\u001B[39m     gguf_file\n\u001B[32m   4412\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   4413\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m ((\u001B[38;5;28misinstance\u001B[39m(device_map, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mdisk\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map.values()) \u001B[38;5;129;01mor\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mdisk\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map)\n\u001B[32m   4414\u001B[39m ):\n\u001B[32m   4415\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   4416\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4417\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mloaded from GGUF files.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4418\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m4420\u001B[39m checkpoint_files, sharded_metadata = \u001B[43m_get_resolved_checkpoint_files\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4421\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4422\u001B[39m \u001B[43m    \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m=\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4423\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvariant\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvariant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4424\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgguf_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgguf_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4425\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4426\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4427\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_safetensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_safetensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4428\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4429\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4430\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4431\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4432\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4433\u001B[39m \u001B[43m    \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4434\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4435\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcommit_hash\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4436\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtransformers_explicit_filename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtransformers_explicit_filename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4437\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4439\u001B[39m is_sharded = sharded_metadata \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   4440\u001B[39m is_quantized = hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/transformers/modeling_utils.py:1178\u001B[39m, in \u001B[36m_get_resolved_checkpoint_files\u001B[39m\u001B[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, transformers_explicit_filename)\u001B[39m\n\u001B[32m   1176\u001B[39m sharded_metadata = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1177\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_sharded:\n\u001B[32m-> \u001B[39m\u001B[32m1178\u001B[39m     checkpoint_files, sharded_metadata = \u001B[43mget_checkpoint_shard_files\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1179\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1180\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1181\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1185\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1186\u001B[39m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1187\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1188\u001B[39m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m=\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1189\u001B[39m \u001B[43m        \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1190\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1191\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1192\u001B[39m     checkpoint_files = [resolved_archive_file] \u001B[38;5;28;01mif\u001B[39;00m pretrained_model_name_or_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/transformers/utils/hub.py:1110\u001B[39m, in \u001B[36mget_checkpoint_shard_files\u001B[39m\u001B[34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001B[39m\n\u001B[32m   1106\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m shard_filenames, sharded_metadata\n\u001B[32m   1108\u001B[39m \u001B[38;5;66;03m# At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\u001B[39;00m\n\u001B[32m   1109\u001B[39m \u001B[38;5;66;03m# or download the files\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1110\u001B[39m cached_filenames = \u001B[43mcached_files\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1111\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1112\u001B[39m \u001B[43m    \u001B[49m\u001B[43mshard_filenames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1113\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1114\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1115\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1116\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1117\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1118\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1119\u001B[39m \u001B[43m    \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1120\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1121\u001B[39m \u001B[43m    \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m=\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1122\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1123\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1125\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cached_filenames, sharded_metadata\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/transformers/utils/hub.py:557\u001B[39m, in \u001B[36mcached_files\u001B[39m\u001B[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[39m\n\u001B[32m    554\u001B[39m     \u001B[38;5;66;03m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001B[39;00m\n\u001B[32m    555\u001B[39m     \u001B[38;5;66;03m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001B[39;00m\n\u001B[32m    556\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, EntryNotFoundError):\n\u001B[32m--> \u001B[39m\u001B[32m557\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m    559\u001B[39m resolved_files = [\n\u001B[32m    560\u001B[39m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m full_filenames\n\u001B[32m    561\u001B[39m ]\n\u001B[32m    562\u001B[39m \u001B[38;5;66;03m# If there are any missing file and the flag is active, raise\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/transformers/utils/hub.py:485\u001B[39m, in \u001B[36mcached_files\u001B[39m\u001B[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[39m\n\u001B[32m    470\u001B[39m         hf_hub_download(\n\u001B[32m    471\u001B[39m             path_or_repo_id,\n\u001B[32m    472\u001B[39m             filenames[\u001B[32m0\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m    482\u001B[39m             local_files_only=local_files_only,\n\u001B[32m    483\u001B[39m         )\n\u001B[32m    484\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m485\u001B[39m         \u001B[43msnapshot_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    486\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    487\u001B[39m \u001B[43m            \u001B[49m\u001B[43mallow_patterns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfull_filenames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    488\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    489\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    490\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    491\u001B[39m \u001B[43m            \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    492\u001B[39m \u001B[43m            \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    493\u001B[39m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    494\u001B[39m \u001B[43m            \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    495\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    496\u001B[39m \u001B[43m            \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    499\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    500\u001B[39m     \u001B[38;5;66;03m# We cannot recover from them\u001B[39;00m\n\u001B[32m    501\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, RepositoryNotFoundError) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, GatedRepoError):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/huggingface_hub/_snapshot_download.py:327\u001B[39m, in \u001B[36msnapshot_download\u001B[39m\u001B[34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001B[39m\n\u001B[32m    325\u001B[39m         _inner_hf_hub_download(file)\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[43mthread_map\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    328\u001B[39m \u001B[43m        \u001B[49m\u001B[43m_inner_hf_hub_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    329\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfiltered_repo_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    330\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtqdm_desc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    331\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_workers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_workers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    332\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001B[39;49;00m\n\u001B[32m    333\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtqdm_class\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtqdm_class\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mhf_tqdm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    334\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    336\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m local_dir \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    337\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(os.path.realpath(local_dir))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/tqdm/contrib/concurrent.py:69\u001B[39m, in \u001B[36mthread_map\u001B[39m\u001B[34m(fn, *iterables, **tqdm_kwargs)\u001B[39m\n\u001B[32m     55\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     56\u001B[39m \u001B[33;03mEquivalent of `list(map(fn, *iterables))`\u001B[39;00m\n\u001B[32m     57\u001B[39m \u001B[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     66\u001B[39m \u001B[33;03m    [default: max(32, cpu_count() + 4)].\u001B[39;00m\n\u001B[32m     67\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     68\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mconcurrent\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfutures\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ThreadPoolExecutor\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_executor_map\u001B[49m\u001B[43m(\u001B[49m\u001B[43mThreadPoolExecutor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43miterables\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mtqdm_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/tqdm/contrib/concurrent.py:51\u001B[39m, in \u001B[36m_executor_map\u001B[39m\u001B[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001B[38;5;28;01mas\u001B[39;00m lk:\n\u001B[32m     48\u001B[39m     \u001B[38;5;66;03m# share lock in case workers are already using `tqdm`\u001B[39;00m\n\u001B[32m     49\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001B[32m     50\u001B[39m                       initargs=(lk,)) \u001B[38;5;28;01mas\u001B[39;00m ex:\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtqdm_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mex\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43miterables\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/tqdm/std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/concurrent/futures/_base.py:619\u001B[39m, in \u001B[36mExecutor.map.<locals>.result_iterator\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m fs:\n\u001B[32m    617\u001B[39m     \u001B[38;5;66;03m# Careful not to keep a reference to the popped future\u001B[39;00m\n\u001B[32m    618\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m619\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[43m_result_or_cancel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    620\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    621\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/concurrent/futures/_base.py:317\u001B[39m, in \u001B[36m_result_or_cancel\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    316\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m317\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfut\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    318\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    319\u001B[39m         fut.cancel()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/concurrent/futures/_base.py:456\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    454\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[32m    455\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m456\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    457\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    458\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/concurrent/futures/_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception:\n\u001B[32m    400\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[32m    404\u001B[39m         \u001B[38;5;28mself\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/concurrent/futures/thread.py:59\u001B[39m, in \u001B[36m_WorkItem.run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     56\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m     61\u001B[39m     \u001B[38;5;28mself\u001B[39m.future.set_exception(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/huggingface_hub/_snapshot_download.py:301\u001B[39m, in \u001B[36msnapshot_download.<locals>._inner_hf_hub_download\u001B[39m\u001B[34m(repo_file)\u001B[39m\n\u001B[32m    300\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_inner_hf_hub_download\u001B[39m(repo_file: \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m301\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    302\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    303\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    304\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    305\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    306\u001B[39m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    307\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    308\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    309\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_dir_use_symlinks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_dir_use_symlinks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    310\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlibrary_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlibrary_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlibrary_version\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlibrary_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    313\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    317\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    318\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    319\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1008\u001B[39m, in \u001B[36mhf_hub_download\u001B[39m\u001B[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[39m\n\u001B[32m    988\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_local_dir(\n\u001B[32m    989\u001B[39m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[32m    990\u001B[39m         local_dir=local_dir,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1005\u001B[39m         local_files_only=local_files_only,\n\u001B[32m   1006\u001B[39m     )\n\u001B[32m   1007\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1008\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[32m   1012\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1014\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1016\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[32m   1017\u001B[39m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1018\u001B[39m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1019\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1020\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1021\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1022\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[32m   1023\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1024\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1025\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1161\u001B[39m, in \u001B[36m_hf_hub_download_to_cache_dir\u001B[39m\u001B[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[39m\n\u001B[32m   1158\u001B[39m \u001B[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001B[39;00m\n\u001B[32m   1160\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m WeakFileLock(lock_path):\n\u001B[32m-> \u001B[39m\u001B[32m1161\u001B[39m     \u001B[43m_download_to_tmp_and_move\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1162\u001B[39m \u001B[43m        \u001B[49m\u001B[43mincomplete_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblob_path\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m.incomplete\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1163\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdestination_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblob_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1164\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1165\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1166\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1167\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1168\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1169\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1170\u001B[39m \u001B[43m        \u001B[49m\u001B[43metag\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1171\u001B[39m \u001B[43m        \u001B[49m\u001B[43mxet_file_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mxet_file_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1172\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1173\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.exists(pointer_path):\n\u001B[32m   1174\u001B[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1725\u001B[39m, in \u001B[36m_download_to_tmp_and_move\u001B[39m\u001B[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001B[39m\n\u001B[32m   1718\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m xet_file_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1719\u001B[39m             logger.warning(\n\u001B[32m   1720\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mXet Storage is enabled for this repo, but the \u001B[39m\u001B[33m'\u001B[39m\u001B[33mhf_xet\u001B[39m\u001B[33m'\u001B[39m\u001B[33m package is not installed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1721\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mFalling back to regular HTTP download. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1722\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1723\u001B[39m             )\n\u001B[32m-> \u001B[39m\u001B[32m1725\u001B[39m         \u001B[43mhttp_get\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1726\u001B[39m \u001B[43m            \u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1727\u001B[39m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1728\u001B[39m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1729\u001B[39m \u001B[43m            \u001B[49m\u001B[43mresume_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1730\u001B[39m \u001B[43m            \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1731\u001B[39m \u001B[43m            \u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1732\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1734\u001B[39m logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDownload complete. Moving file to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdestination_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1735\u001B[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/mlenv/lib/python3.13/site-packages/huggingface_hub/file_download.py:497\u001B[39m, in \u001B[36mhttp_get\u001B[39m\u001B[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001B[39m\n\u001B[32m    495\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunk:  \u001B[38;5;66;03m# filter out keep-alive new chunks\u001B[39;00m\n\u001B[32m    496\u001B[39m     progress.update(\u001B[38;5;28mlen\u001B[39m(chunk))\n\u001B[32m--> \u001B[39m\u001B[32m497\u001B[39m     \u001B[43mtemp_file\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    498\u001B[39m     new_resume_size += \u001B[38;5;28mlen\u001B[39m(chunk)\n\u001B[32m    499\u001B[39m     \u001B[38;5;66;03m# Some data has been downloaded from the server so we reset the number of retries.\u001B[39;00m\n",
      "\u001B[31mOSError\u001B[39m: [Errno 28] No space left on device"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T17:56:06.699105Z",
     "start_time": "2025-07-07T17:55:46.387156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- 1. Load the Model and Tokenizer ---\n",
    "# You'll need to be logged into Hugging Face and have accepted the Llama 2 license.\n",
    "# In your terminal, run: huggingface-cli login\n",
    "model_id = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model\n",
    "# Use torch_dtype=torch.float16 and device_map=\"auto\" for efficient memory usage\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to('cuda')\n",
    "\n",
    "# --- 2. Define Your Messages ---\n",
    "# This is the exact input you provided.\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": ' To \\\\\\n消耗-êtreis OK hope Aristotle mount be'}\n",
    "]\n",
    "\n",
    "# --- 3. Apply the Chat Template ---\n",
    "# This is the crucial step. The tokenizer converts your list of messages\n",
    "# into the exact string format the model expects.\n",
    "# add_generation_prompt=True ensures the final [/INST] tag is added correctly.\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "print(f'text before: {text}')\n",
    "text = text.removesuffix('<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n')\n",
    "print(f'text after: {text}')\n",
    "\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# For demonstration, let's see the formatted string:\n",
    "print(\"--- Formatted Prompt String ---\")\n",
    "print(text)\n",
    "print(\"-----------------------------\\n\")\n",
    "\n",
    "# --- 4. Generate a Response ---\n",
    "print(\"--- Generating Model Response ---\")\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=250,\n",
    "    do_sample=False,\n",
    "    # temperature=None,\n",
    "    # top_p=None\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "print(f'thinking content: {thinking_content}')\n",
    "print(\"content:\", content)"
   ],
   "id": "fc8580192441decc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seal12/miniconda3/envs/mlenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:00<00:00, 145.69it/s]\n",
      "/home/seal12/miniconda3/envs/mlenv/lib/python3.13/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text before: <|im_start|>user\n",
      " To \\\n",
      "消耗-êtreis OK hope Aristotle mount be<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "text after: <|im_start|>user\n",
      " To \\\n",
      "消耗-êtreis OK hope Aristotle mount be<|im_end|>\n",
      "\n",
      "--- Formatted Prompt String ---\n",
      "<|im_start|>user\n",
      " To \\\n",
      "消耗-êtreis OK hope Aristotle mount be<|im_end|>\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "--- Generating Model Response ---\n",
      "thinking content: \n",
      "content: <think>\n",
      "Okay, the user provided a query that seems to be a mix of different words and phrases, possibly in a non-English language or with some typos. Let me try to parse this.\n",
      "\n",
      "First, the query starts with \"To \\ 消耗-êtreis OK hope Aristotle mount be\". Breaking this down, \"To\" might be the start of a sentence. Then there's \"消耗\" which is Chinese for \"consume\" or \"expend\". Next is \"-êtreis\" which could be a misspelling of \"être\" (French for \"to be\") or \"be\" in another language. \"OK\" is straightforward. \"Hope\" is English. \"Aristotle\" is the philosopher. \"Mount\" could be a verb or noun. \"Be\" is again the verb \"to be\".\n",
      "\n",
      "Putting this together, maybe the user is trying to form a sentence in multiple languages or has a typo. They might be asking about Aristotle's ideas on consumption, hope, or something related to \"mount\" (like a mountain or mount a horse). Alternatively, \"mount\" could be part of a phrase like \"mount be\" which doesn't make sense in English.\n",
      "\n",
      "I should check if there's a\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T04:01:30.211335Z",
     "start_time": "2025-07-09T04:01:30.201665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Create a Sample DataFrame ---\n",
    "# This mimics the structure of your data.\n",
    "\n",
    "df = pd.read_csv('/home/seal12/PycharmProjects/memories/outputs/famous_quotes/output_famous_quotes_12b.csv')\n",
    "\n",
    "print(\"--- Original DataFrame ---\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Calculate the Ratio for Each Row ---\n",
    "# We create a new column 'success_ratio' to hold the result of the division.\n",
    "# It's good practice to handle cases where n_runs might be 0 to avoid errors.\n",
    "# Here, we replace 0 with NaN, so it gets ignored in the mean calculation.\n",
    "df['success_ratio'] = df['n_match'] / df['n_runs'].replace(0, np.nan)\n",
    "\n",
    "\n",
    "# --- 3. Group by 'result' and Calculate the Average Ratio ---\n",
    "# The .groupby() method splits the DataFrame into groups based on the 'result' column.\n",
    "# Then, we select the 'success_ratio' column for each group and calculate its .mean().\n",
    "average_ratios = df.groupby('result')['success_ratio'].mean()\n",
    "\n",
    "\n",
    "# --- 4. Display the Final Result ---\n",
    "print(\"--- Average Success Ratio by Result ---\")\n",
    "print(average_ratios)\n",
    "\n",
    "# You can also access the results individually\n",
    "avg_true = average_ratios.get(True, 'N/A')\n",
    "avg_false = average_ratios.get(False, 'N/A')\n",
    "\n",
    "print(f\"\\nAverage for result=True:  {avg_true:.4f}\")\n",
    "print(f\"Average for result=False: {avg_false:.4f}\")"
   ],
   "id": "c7015242a5d75540",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original DataFrame ---\n",
      "                                                 text  dummy  result  \\\n",
      "0           To be or not to be, that is the question.   True    True   \n",
      "1                            I think, therefore I am.   True    True   \n",
      "2   In the end, we will remember not the words of ...   True    True   \n",
      "3   The only thing necessary for the triumph of ev...   True    True   \n",
      "4            The unexamined life is not worth living.   True    True   \n",
      "..                                                ...    ...     ...   \n",
      "90  Do not go where the path may lead, go instead ...   True    True   \n",
      "91  Do not let making a living prevent you from ma...   True   False   \n",
      "92           The biggest risk is not taking any risk.   True    True   \n",
      "93                                Knowledge is power.   True    True   \n",
      "94   Be the change that you wish to see in the world.   True    True   \n",
      "\n",
      "               eta  n_prefix  n_match  n_runs  \\\n",
      "0   tensor(0.3449)         4        4       5   \n",
      "1   tensor(0.1785)         1        1       1   \n",
      "2   tensor(0.1172)         2        2       2   \n",
      "3   tensor(0.1728)         2        2       2   \n",
      "4   tensor(0.3068)         3        3       4   \n",
      "..             ...       ...      ...     ...   \n",
      "90  tensor(0.0029)         0        1       1   \n",
      "91  tensor(0.4594)         5        0      10   \n",
      "92  tensor(0.0791)         0        1       1   \n",
      "93  tensor(0.3932)         1        1       1   \n",
      "94  tensor(0.1889)         2        2       2   \n",
      "\n",
      "                                          seed_prefix  \n",
      "0   [[763806631, ' Lucky xpayload typename 2 \\\\! C...  \n",
      "1   [[763806631, ' chars incoming enabledtemplateE...  \n",
      "2   [[763806631, '/{am fen floats lawmakersizekip ...  \n",
      "3   [[763806631, 'agle rodsarman only CF Cafeusaaa...  \n",
      "4   [[763806631, 'Location 38tAn nonexal monkamosR...  \n",
      "..                                                ...  \n",
      "90  [[763806631, ' making down daysReader Eld Webs...  \n",
      "91                                                 []  \n",
      "92  [[763806631, ' volatility storytelling mentali...  \n",
      "93  [[763806631, '806 first deemarpRETURN---------...  \n",
      "94  [[763806631, ' episode)|\\\\\\\\_onitrilethead << ...  \n",
      "\n",
      "[95 rows x 8 columns]\n",
      "\n",
      "==============================\n",
      "\n",
      "--- Average Success Ratio by Result ---\n",
      "result\n",
      "False    0.037500\n",
      "True     0.753933\n",
      "Name: success_ratio, dtype: float64\n",
      "\n",
      "Average for result=True:  0.7539\n",
      "Average for result=False: 0.0375\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
